{"cells":[{"cell_type":"markdown","metadata":{"id":"ZCWSEtNeGMsN"},"source":["---\n","\n","This project involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. I will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n","\n","\n","Download train.csv, test_seen.csv and test_unseen.csv from the Github Suggestion mining folder or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n","\n","You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n","](https://aclanthology.org/S19-2151/)."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BSCPHJ3v76q","executionInfo":{"status":"ok","timestamp":1674217533493,"user_tz":0,"elapsed":34180,"user":{"displayName":"mahima sharm","userId":"00896754877636954831"}},"outputId":"ec13f45d-7092-4572-ff60-b4a9b660ea35"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!curl \"https://raw.githubusercontent.com/mahima-sharma10/Suggestion-mining/main/train.csv\" > train.csv\n","!curl \"https://raw.githubusercontent.com/mahima-sharma10/Suggestion-mining/main/test_seen.csv\" > test.csv"],"metadata":{"id":"cwn6n72ukdc7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674217534043,"user_tz":0,"elapsed":557,"user":{"displayName":"mahima sharm","userId":"00896754877636954831"}},"outputId":"7580511a-4c0d-49e8-818a-df54f5c0a6aa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  670k  100  670k    0     0  1610k      0 --:--:-- --:--:-- --:--:-- 1610k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  168k  100  168k    0     0   505k      0 --:--:-- --:--:-- --:--:--  503k\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5x0c38rCGk23","executionInfo":{"status":"ok","timestamp":1674217535185,"user_tz":0,"elapsed":1144,"user":{"displayName":"mahima sharm","userId":"00896754877636954831"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Read the CSV file.\n","train_df = pd.read_csv('train.csv', \n","                 names=['id', 'text', 'label'], header=0)\n","\n","test_df = pd.read_csv('test.csv', \n","                 names=['id', 'text', 'label'], header=0)\n","\n","# Store the data as a list of tuples where the first item is the text\n","# and the second item is the label.\n","train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n","test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n","\n","# Check that training set and test set are of the right size.\n","assert len(test_texts) == len(test_labels) == 1360\n","assert len(train_texts) == len(train_labels) == 5440"]},{"cell_type":"markdown","metadata":{"id":"I_Scj45oSpdQ"},"source":["---\n","\n","## Task 1: Data Pre-processing "]},{"cell_type":"markdown","metadata":{"id":"8Pd8ed8NdlB_"},"source":["\n","\n","Edit this cell to write your answer below the line in no more than 300 words.\n","\n","---\n","\n",">\n","1st lowercase = Converting a word to lower case (NLP -> nlp). Words like Book and book mean the same but when not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimensions). The lower() function makes the whole process quite straightforward.\n","Also, when the text is lower cased, it treats the complete text equal and easy to process.\n","\n","2nd Removed punctuation  = The punctuation to the sentence adds up noise that brings ambiguity while training the model. Hence, I have added a block below to remove the punctuations from the text.\n","\n","3rd Removed hyperlinks = It is important to remove the hyperlinks from the text as it is an added information or references which hardly contribute and also have low occurence and removing them might help in getting a good accuracy score."]},{"cell_type":"markdown","metadata":{"id":"t2-xXQggaVKh"},"source":["In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."]},{"cell_type":"code","source":["import nltk\n","nltk.download(['punkt', 'wordnet', \"omw-1.4\", 'averaged_perceptron_tagger', 'universal_tagset','stopwords' ])\n","import nltk,csv,numpy,re \n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","\n","#lowercasefortraindata\n","words_train = []\n","for word in train_texts:\n","  words_train.append(word.lower())\n","#lowercasefortestdata\n","words_test = []\n","for text in test_texts:\n","  words_test.append(text.lower())\n","#Removing hyperlink for train data\n","reg_train = []\n","for word in words_train:\n","  reg_train.append(re.sub(r\"http\\S+\", \"\", word))\n","#Removing hyperlink for test data\n","reg_test = []\n","for text in words_test:\n","  reg_test.append(re.sub(r\"http\\S+\", \"\", text))\n","#Removing punctuations for train data\n","punct_train = []\n","for word in reg_train:\n","  punct_train.append(re.sub(r'[^\\w\\s]','',word))\n","#Removing punctuations for test data\n","punct_test = []\n","for text in reg_test:\n","  punct_test.append(re.sub(r'[^\\w\\s]','',text))\n","#defined a function names myfunction to tokenize and join the data to make it processable for count vectorizer\n","def myfunction(z):\n","  text_lower = [w.lower() for w in z]\n","  tokenized_sents = [word_tokenize(i) for i in text_lower]\n","  \n","  for i in range(len(tokenized_sents)):\n","    tokenized_sents[i] = ' '.join(tokenized_sents[i])\n","    processed_texts = tokenized_sents\n","    \n","  return processed_texts\n","#saving the tokenized and joined data for training data\n","processed_texts = myfunction(punct_train)\n","train_df['processed_texts'] = processed_texts\n","trainX = train_df['processed_texts']\n","##saving the tokenized and joined data for training data\n","processed_texts = myfunction(punct_test)\n","test_df['processed_texts'] = processed_texts\n","testX = test_df['processed_texts']"],"metadata":{"id":"v1PCZ332kJ0N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"151c1403-e221-4cd9-859a-ab9a9568c9d8","executionInfo":{"status":"ok","timestamp":1674217537345,"user_tz":0,"elapsed":2163,"user":{"displayName":"mahima sharm","userId":"00896754877636954831"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"3IUJunnfXItQ"},"source":["---\n","\n","## Task 2: Feature Engineering (I) - TF-IDF as features\n","\n","We have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. In the following code cell, created a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n","\n","After applying preprocessing steps, used the training data to train the classifier and make predictions on the test set. \n","\n","If everything is implemented correctly, then we should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gDsfB8xTGMg"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Calculate tf-idf scores for the words in the training set.\n","# ... your code goes here \n","#generate the features for the train set\n","#count_vect = CountVectorizer(analyzer='word',token_pattern=r'\\b[a-zA-Z]{3,}\\b',ngram_range=(1, 1))\n","count_vect = CountVectorizer(ngram_range=(1, 3))  \n","X_train_counts = count_vect.fit_transform(trainX)\n","tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","\n","#generate the features for the test set\n","X_test_counts = count_vect.transform(testX)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n","\n","# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n","# ... your code goes here\n","# Lets train a Gaussian Naive Bayes clasifier using counts \n","NB_classifier_counts = GaussianNB()\n","NB_classifier_counts.fit(X_train_counts.toarray(), train_labels)\n","\n","\n","# Predict on the test set.\n","#predictions = []    # save your predictions on the test set into this list\n","predictions = NB_classifier_counts.predict(X_test_counts.toarray())\n","\n","# ... your code goes here\n","\n","#################### DO NOT EDIT BELOW THIS LINE #################\n","\n","\n","#################### DO NOT EDIT BELOW THIS LINE #################\n","\n","def accuracy(labels, predictions):\n","  '''\n","  Calculated the accuracy score for a given set of predictions and labels.\n","  \n","  Args:\n","    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n","    predictions (list): A list containing predictions annotated as `0` and `1`.\n","\n","  Returns:\n","    float: A floating point value to score the predictions against the labels.\n","  '''\n","\n","  assert len(labels) == len(predictions)\n","  \n","  correct = 0\n","  for label, prediction in zip(labels, predictions):\n","    if label == prediction:\n","      correct += 1 \n","  \n","  score = correct / len(labels)\n","  return score\n","\n","# Calculate accuracy score for the classifier using tf-idf features.\n","accuracy(test_labels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"DDx_M2aTIncl"},"source":["---\n","\n","## Task 3: Evaluation Metrics\n","\n","Accuracy not the best measure for evaluating a classifier? Described an evaluation metric which might work better than accuracy for a classification task such as suggestion detection."]},{"cell_type":"markdown","metadata":{"id":"M8jDzSU86xI1"},"source":["Accuracy scores can be misleading as it can hide the imbalanced dataset issue.\n","An evaluation that might work better than accuracy for a classification task is F1 score.\n","Also, in the below code, I have calculated the F1- score of the trained model to evaluate the model. In addition to this, these are points adding to why f1 score is better than accuracy score.\n","1. Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial\n","2. Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.\n","3. In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on. \n","---"]},{"cell_type":"markdown","metadata":{"id":"2ozD4SyyRDL3"},"source":["In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UkUX5K0oMhKI"},"outputs":[],"source":["def evaluate(labels, predictions):\n","  '''\n","  Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n","  \n","  Args:\n","    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n","    predictions (list): A list containing predictions annotated as `0` and `1`.\n","\n","  Returns:\n","    float: A floating point value to score the predictions against the labels.\n","  '''\n","  \n","  # check that labels and predictions are of same length\n","  assert len(labels) == len(predictions)\n","\n","  score = 0.0\n","  \n","  #################### EDIT BELOW THIS LINE #########################\n","\n","  # your code goes here\n","  def confusionmatrix(labels, predictions):\n","    tp = 0\n","    for lbl, pred in zip(labels, predictions):\n","        if lbl == 1 and pred == 1:\n","            tp +=1\n","    tn = 0\n","    for lbl, pred in zip(labels, predictions):\n","        if lbl == 0 and pred == 0:\n","            tn +=1\n","    fp = 0\n","    for lbl, pred in zip(labels, predictions):\n","        if lbl == 0 and pred == 1:\n","            fp +=1\n","    fn = 0\n","    for lbl, pred in zip(labels, predictions):\n","        if lbl == 1 and pred == 0:\n","            fn +=1\n","    \n","    precision = tp/ (tp + fp)  \n","    recall = tp/ (tp + fn)  \n","\n","    p = precision\n","    r = recall\n","    final_score = 2 * p * r/ (p + r) \n","    return final_score\n","  \n","  final_score=confusionmatrix(labels, predictions)\n","\n","  #################### EDIT ABOVE THIS LINE #########################\n","\n","  return final_score\n","\n","# Calculate evaluation score based on the metric of your choice\n","# for the classifier trained in Task 2 using tf-idf features.\n","evaluate(test_labels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"22OelF89a27J"},"source":["---\n","\n","## Task 4: Feature Engineering (II) - Other features \n","\n","Described features other than those defined in Task 2 which might improve the performance of your suggestion detector.\n"]},{"cell_type":"markdown","metadata":{"id":"4EBS0F877UyC"},"source":["Edit this cell to write your answer below the line in no more than 500 words.\n","\n","---\n","  \n","> I have added a preprocessing step where I have removed the stopwords from the training and testing data and used the cleaned data for generating the tfidf scores through tfidf vectorization and trained the data on bernoulli model which has slightly improved the performance of the model.\n"," \n","  \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"yfkzM3DRce14"},"source":["In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n","\n","After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 3. You **must not** use the test set for training.\n","\n","To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."]},{"cell_type":"code","source":["#stopword removal\n","stops = set(stopwords.words('english'))\n","\n","text_clean = [word for word in punct_train if word not in stops]\n","\n","processed_texts = myfunction(text_clean)\n","train_stpwrdX = train_df['processed_texts']\n","test_stpwrdX = test_df['processed_texts']\n","\n","# Create your features.\n","# ... your code goes here\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","X_train_tfidf_vec = tfidf_vectorizer.fit_transform(train_stpwrdX)\n","print(X_train_tfidf_vec.toarray())\n","tfidf_vectorizer.get_feature_names_out().tolist()\n","\n","#generate the features for the test set\n","X_test_vec = tfidf_vectorizer.transform(test_stpwrdX)\n","\n","# Train a Naïve Bayes classifier using the features you defined.\n","# ... your code goes here\n","from sklearn.naive_bayes import BernoulliNB\n","NB_classifier_tfidf = BernoulliNB()\n","NB_classifier_tfidf.fit(X_train_tfidf_vec.toarray(), train_labels)\n","\n","# Evaluate on the test set.\n","# ... your code goes here\n","from sklearn.metrics import classification_report\n","preds = NB_classifier_tfidf.predict(X_test_vec.toarray())\n","print(classification_report(test_labels, preds))"],"metadata":{"id":"eaKQEAvK7SS8"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}